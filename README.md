# ğŸ“š StoriÂ Genâ€‘AI Challenge â€“ Mexicanâ€¯Revolution RAG Assistant

A lightweight **Retrievalâ€‘Augmented Generation (RAG)** chatbot that answers questions about the Mexican Revolution using local, openâ€‘source models â€“ no external APIs required.

---

## ğŸš€ Key Features

| Module            | Tech                                     | Purpose                                |
| ----------------- | ---------------------------------------- | -------------------------------------- |
| **Vectorâ€¯Store**  | FAISS + `mpnet-base-dot-v1`              | Fast semantic retrieval of PDF chunks  |
| **Generator**     | `google/flanâ€‘t5â€‘base` + **LoRA** adapter | Lowâ€‘cost local text generation         |
| **Semantic Gate** | Globalâ€‘vector cosineâ€¯â‰¥â€¯0.40              | Blocks offâ€‘topic questions             |
| **Summarizer**    | `facebook/bart-large-cnn`                | Compresses long answers / chat history |
| **API**           | FastAPI (`/ask`, `/summarize`)           | Simple REST interface                  |
| **GUI**           | Streamlit oneâ€‘pager                      | Chat + â€œSummarizeâ€ button              |
| **Container**     | Docker (PythonÂ 3.10â€‘slim)                | Oneâ€‘shot build & run                   |

---

## ğŸ—‚ï¸ Project Layout

```
stori-genai-challenge/
â”œâ”€â”€ app/                  # FastAPI + RAG logic
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build_index.py    # Build FAISS index from PDF
â”œâ”€â”€ data/                 # mexican_revolution.pdf
â”œâ”€â”€ embeddings/           # faiss_index/ + global_embedding.pkl
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md (this file)
```

---

## âš™ï¸ Local Setup (devâ€‘mode)

```bash
# 1Â Clone repo
$ git clone <repo-url> && cd stori-genai-challenge

# 2Â Create venv & install deps
$ python -m venv venv
$ source venv/bin/activate
$ pip install -r requirements.txt

# 3Â Build FAISS index (once)
$ python scripts/build_index.py

# 4Â Run API
$ uvicorn app.main:app --reload  # http://localhost:8000/

# 5Â Run GUI (optional)
$ streamlit run app/gui.py       # http://localhost:8501
```

---

## ğŸ³Â Docker Usage (recommended)

The Dockerfile already calls `scripts/build_index.py` during the image build.

```bash
# Build image
$ docker build -t stori-rag .

# Run container
$ docker run -p 8000:8000 stori-rag
# â†’ Swagger UI at http://localhost:8000/docs
```

> **Tip**Â : Mount a host volume if you want the index to persist and skip the build step at container startâ€‘time.

---

## ğŸ§ª Endpoints

| Method | Path        | Body / Query                                             | Description                                 |
| ------ | ------------| -------------------------------------------------------- | --------------------------------------------|
| `POST` | `/ask`      | `{ "question": "Â¿CuÃ¡les fueron las causas polÃ­ticas?" }` | Returns answer or rejection if offâ€‘topic    |
| `GET`  | `/summarize`| â€”                                                        | Summarizes conversation so far              |

---

## âš–ï¸ Design Notes (tradeâ€‘offs)

* **Local models**Â â†’ free, offline, but slower than remote GPTâ€‘3.5.
* **LoRA fineâ€‘tune (1â€¯000 synthetic rows)**Â â†’ quick accuracy boost without full retrain.
* **Single container**Â â†’ minimal ops; FAISS index baked at build time.
* **CPUâ€‘only**Â â†’ runs everywhere; heavy (>7â€¯B) models not feasible.

---

## ğŸŒ± Future Work

* Hybrid BM25 + dense retrieval
* Streamed token output (SSE)
* Automated RAG eval (LangChain QAGold)
* Persist embeddings on volume / DB to speed container startups

---

Made with â¤ï¸Â for Storiâ€™s Generative AI Challenge
